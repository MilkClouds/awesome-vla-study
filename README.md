# Awesome VLA Study

A 13-week study guide for **Vision-Language-Action (VLA) models** â€” 28 papers + 1 course across 6 phases, ordered so each week builds on the last.

### Prerequisites
- Deep learning fundamentals (Transformers, attention, tokenization)
- Basic probability & optimization (enough to follow ELBO, score matching derivations)

### Weekly Format (Recommended)
- **Paper presentation**: 1â€“2 participants per week, 30 min/paper â€” architecture, training, key results
- **Discussion**: Compare design choices across the week's papers, discuss limitations and open questions (15â€“20 min)

| Phase | Weeks | Topic | Readings |
|-------|-------|-------|----------|
| **Phase 1** | W1â€“3 | Generative Model Foundations | MIT 6.S184 course |
| **Phase 2** | W4â€“5 | Early Foundation RFMs & Robot Policy | RT-1, RT-2, Octo, OpenVLA, BeT, Diffusion Policy, ACT |
| **Phase 3** | W6â€“7 | Current RFM Architectures | CogACT, GR00T N1, X-VLA, Ï€0, InternVLA-M1 |
| **Phase 4** | W8â€“9 | Data Scaling | OXE, AgiBot World, UMI, VITRA, Human to Robot Transfer |
| **Phase 5** | W10â€“11 | Efficient Inference & Dual-System | RTC, SmolVLA, Helix, Fast-in-Slow |
| **Phase 6** | W12â€“13 | RL Fine-tuning & World Model | HIL-SERL, SimpleVLA-RL, Ï€\*0.6, CoT-VLA, UniVLA, Cosmos Policy, DreamZero |

---

## Phase 1: Generative Model Foundations (Weeks 1â€“3)

**ðŸ“š Core Material**: [MIT 6.S184 â€” Introduction to Flow Matching and Diffusion Models](https://diffusion.csail.mit.edu/2025/index.html) (Holderrieth & Erives, MIT CSAIL, 2025) | [Course notes paper](https://arxiv.org/abs/2506.02070)

### Week 1: ODE/SDE Foundations & Diffusion Models
| Material | Topic |
|----------|-------|
| Lectures 1â€“2 | ODE/SDE basics, forward/reverse processes, conditional/marginal probability paths |
| Lab 1 | Hands-on SDE simulation |

### Week 2: Flow Matching, Score Matching & Training
| Material | Topic |
|----------|-------|
| Lectures 3â€“4 | Flow Matching, Score Matching, guidance, classifier-free guidance |
| Labs 2â€“3 | Building a toy diffusion model from scratch |

### Week 3: Generative Robotics & Review
| Material | Topic |
|----------|-------|
| Lecture 5 | Guest lecture by Benjamin Burchfiel (Toyota Research): diffusion models for robotics |
| Lecture 6 | Generative protein design (optional) |



---

## Phase 2: Early Foundation Robot Models & Robot Policy (Weeks 4â€“5)

### Week 4: Early Foundation Robot Models â€” RT-1, RT-2, Octo, OpenVLA
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 1 | **RT-1: Robotics Transformer** â€” Brohan et al. (2022) | [2212.06817](https://arxiv.org/abs/2212.06817) | First large-scale Robotics Transformer (no VLM) |
| 2 | **RT-2: Vision-Language-Action Models** â€” Brohan et al. (2023) | [2307.15818](https://arxiv.org/abs/2307.15818) | VLM backbone â†’ VLA paradigm |
| 3 | **Octo** â€” Ghosh et al. (2024) | [2405.12213](https://arxiv.org/abs/2405.12213) | Lightweight open-source diffusion policy (no VLM) |
| 4 | **OpenVLA** â€” Kim et al. (2024) | [2406.09246](https://arxiv.org/abs/2406.09246) | First open-source VLM-based VLA |

> ðŸ“Ž Supplementary video: [Stanford CS25 V3 â€” Low-level Embodied Intelligence](https://www.youtube.com/watch?v=fz8wf9hN20c)

**Key points**: RT-1 (35M, no VLM) â†’ RT-2 (55B VLM, action as text tokens) establishes the VLA concept. Octo (27Mâ€“93M, diffusion head, no VLM) and OpenVLA (7B, VLM + 256-bin discretization) are the first open-source generalist robot policies enabling community iteration.

### Week 5: Core Robot Policies â€” Diffusion Policy, ACT, BeT
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 5 | **Behavior Transformers (BeT)** â€” Shafiullah et al. (2022) | [2206.11251](https://arxiv.org/abs/2206.11251) | Multimodal action discretization, k-means + offset |
| 6 | **Diffusion Policy** â€” Chi et al. (2023) | [2303.04137](https://arxiv.org/abs/2303.04137) | Diffusion for robot control, action chunking |
| 7 | **ACT/ALOHA** â€” Zhao et al. (2023) | [2304.13705](https://arxiv.org/abs/2304.13705) | Action Chunking Transformer, CVAE, bimanual |

**Key points**: Three approaches to the multimodal action problem. Action chunking (predicting K future actions at once) is foundational for later VLA work.

---

## Phase 3: Current RFM Architectures (Weeks 6â€“7)

### Week 6: VLM + Action Head â€” CogACT, GR00T N1, X-VLA
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 8 | **CogACT** â€” Li et al. (2024) | [2411.19650](https://arxiv.org/abs/2411.19650) | VLM + DiT action head, 55%â†‘ vs OpenVLA on real robot |
| 9 | **GR00T N1** â€” Bjorck et al. (2025) | [2503.14734](https://arxiv.org/abs/2503.14734) | 2B diffusion transformer, whole-body humanoid control |
| 10 | **X-VLA** â€” Zheng et al. (2025) | [2510.10274](https://arxiv.org/abs/2510.10274) | Soft prompts for cross-embodiment, Florence-Large + flow matching |

**Key points**: All three use only the VLM's last hidden state to drive a separate action head.

### Week 7: VLM + Action Expert â€” Ï€0, InternVLA-M1
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 11 | **Ï€0** â€” Black et al. (2024) | [2410.24164](https://arxiv.org/abs/2410.24164) | Flow matching + action expert accessing VLM intermediate features |
| 12 | **InternVLA-M1** â€” Chen et al. (2025) | [2510.13778](https://arxiv.org/abs/2510.13778) | Spatial grounding â†’ action generation, AR-based |

> ðŸ“Ž Background: **Transfusion** â€” Zhou et al. (2024) | [2408.11039](https://arxiv.org/abs/2408.11039) â€” AR + diffusion in one transformer; Ï€0's architectural basis

**Key points**: Unlike Week 6's action heads that only see the VLM's last hidden state, these action experts access VLM internal hidden states.

---

## Phase 4: Data Scaling (Weeks 8â€“9)

### Week 8: Large-Scale Robot Datasets â€” OXE, AgiBot World
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 13 | **Open X-Embodiment (OXE)** â€” Open X-Embodiment Collaboration (2023) | [2310.08864](https://arxiv.org/abs/2310.08864) | 1M+ trajectories, 22 embodiments, standardized data format |
| 14 | **AgiBot World** â€” Bu et al. (2025) | [2503.06669](https://arxiv.org/abs/2503.06669) | 1M+ trajectories, 217 tasks, 100+ scenarios |

> ðŸ“Ž **Data formats** â€” Recording-oriented: [rosbag](http://wiki.ros.org/rosbag) (ROS 1), [mcap](https://mcap.dev/) (vendor-neutral, ROS 2 default). Training-oriented: [RLDS](https://github.com/google-research/rlds) (TensorFlow/OXE standard), [LeRobotDataset](https://github.com/huggingface/lerobot) (HuggingFace, Parquet + video).  
> ðŸ“Ž [From the Evolution of Rosbag to the Future of AI Tooling](https://rerun.io/blog/rosbag) â€” by the original rosbag author; covers rosbag V1â†’V2 â†’ rosbag2 (sqlite3) â†’ MCAP evolution

**Key points**: Large-scale multi-embodiment datasets that enable generalist robot policy pretraining. OXE standardized the data format across 22 robot embodiments via RLDS; AgiBot World provides high-quality data at scale.

### Week 9: Data Collection Methods â€” UMI, VITRA, Human to Robot Transfer
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 15 | **UMI** â€” Chi et al. (2024) | [2402.10329](https://arxiv.org/abs/2402.10329) | Robot-free SE(3) data collection via handheld gripper |
| 16 | **VITRA** â€” Li et al. (2025) | [2510.21571](https://arxiv.org/abs/2510.21571) | Human video â†’ VLA training data (1M episodes from Ego4D) |
| 17 | **Human to Robot Transfer** â€” Kareer et al. (2025) | [2512.22414](https://arxiv.org/abs/2512.22414) | Human video â†’ robot transfer emerges with VLA scaling |

**Key points**: Three data sources beyond robot teleoperation â€” UMI (embodiment-agnostic physical demos, <$200 hardware), egocentric video, and exocentric video.

---

## Phase 5: Efficient Inference & Dual-System (Weeks 10â€“11)

### Week 10: Fast-Acting VLA â€” SmolVLA & RTC
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 18 | **SmolVLA** â€” Shukor et al. (2025) | [2506.01844](https://arxiv.org/abs/2506.01844) | 450M params (~1/7 of Ï€0), model compression + async inference |
| 19 | **RTC** â€” Black et al. (2025) | [2506.07339](https://arxiv.org/abs/2506.07339) | Async inference â€” freezing + inpainting, no retraining needed |

**Key points**: Two complementary approaches â€” SmolVLA compresses the model itself, RTC optimizes the inference pipeline. Can be combined.

### Week 11: Dual-System VLA â€” Helix & Fast-in-Slow
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 20 | **Helix** â€” Figure AI (2025) | [figure.ai/news/helix](https://www.figure.ai/news/helix) | S2: 7B VLM @7-9Hz, S1: 80M @200Hz, humanoid |
| 21 | **Fast-in-Slow** â€” Chen et al. (2025) | [2506.01953](https://arxiv.org/abs/2506.01953) | Integrated dual-system, end-to-end trainable |

**Key points**: Dual-System separates slow reasoning (VLM) from fast execution (lightweight policy) at different frequencies. Helix (separately trained) vs Fast-in-Slow (end-to-end trainable).

---

## Phase 6: RL Fine-tuning & World Model (Weeks 12â€“13)

### Week 12: RL Fine-tuning & Human-in-the-Loop â€” HIL-SERL, SimpleVLA-RL, Ï€*0.6
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 22 | **HIL-SERL** â€” Luo et al. (2024) | [2410.21845](https://arxiv.org/abs/2410.21845) | Human-in-the-loop RL, 1-2.5h real-world training, near-perfect success |
| 23 | **SimpleVLA-RL** â€” Li et al. (2025) | [2509.09674](https://arxiv.org/abs/2509.09674) | RL fine-tuning for AR-based VLA, 99.1% LIBERO SOTA |
| 24 | **Ï€\*0.6 / Recap** â€” Physical Intelligence (2025) | [2511.14759](https://arxiv.org/abs/2511.14759) | RL for flow-based VLA, 3-stage pipeline, 90%+ on real tasks |

**Key points**: Three RL approaches â€” HIL-SERL (human-in-the-loop, sample-efficient), SimpleVLA-RL (outcome rewards), Ï€\*0.6 (advantage-conditioned, learns from suboptimal data).

### Week 13: World Model + Reasoning VLA â€” CoT-VLA, UniVLA, Cosmos Policy, DreamZero
| # | Paper | Link | Key Topic |
|---|-------|------|-----------|
| 25 | **CoT-VLA** â€” Zhao et al. (2025) | [2503.22020](https://arxiv.org/abs/2503.22020) | Visual chain-of-thought reasoning (future image prediction) before action |
| 26 | **UniVLA** â€” Wang et al. (2025) | [2506.19850](https://arxiv.org/abs/2506.19850) | Unified AR VLA with world modeling as training objective |
| 27 | **Cosmos Policy** â€” Kim et al. (2026) | [2601.16163](https://arxiv.org/abs/2601.16163) | Video foundation model â†’ robot policy, LIBERO 98.5% |
| 28 | **DreamZero** â€” Ye et al. (2026) | [dreamzero0.github.io](https://dreamzero0.github.io/) | World Action Model, 2Ã— zero-shot generalization vs SOTA VLAs |

**Key points**: All four leverage future prediction for better actions. CoT-VLA (visual reasoning before action), UniVLA (world modeling as training signal), Cosmos Policy (pretrained video model â†’ policy), DreamZero (joint world+action prediction).

---

## See Also

- [Awesome-RL-VLA](https://github.com/Denghaoyuan123/Awesome-RL-VLA) â€” RL for VLA models
- [Awesome-VLA-Robotics](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics) â€” Large-scale VLA paper collection
